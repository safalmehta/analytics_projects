# Read in the training data
mydata <- read.csv("jan2012-April2015.csv")
#mydata
#str(mydata$Category)
mydata <- read.csv("jan2012-April2015.csv")
#mydata
str(mydata$Category)
unique(mydata$Category)
table(mydata$Category)
mydata_hf <- mydata[(mydata$Category == "BURGLARY" | mydata$Category == "ASSAULT" | mydata$Category =='VANDALISM'),] #mydata$Category =='LARCENY/THEFT',]#
mydata_hf_knn <- data.frame(mydata_hf$Category,mydata_hf$X,mydata_hf$Y)
colnames(mydata_hf_knn) <- c("Category","X","Y")
colnames(mydata_hf_knn)
#mydata_hf_knn
set.seed(100) # Set Seed so that same sample can be reproduced in future also
n = nrow(mydata_hf_knn)
trainIndex = sample(1:n, size = round(0.80*n), replace=FALSE)
train = mydata_hf_knn[trainIndex ,]
test = mydata_hf_knn[-trainIndex ,]
table(mydata$Category)
unique(mydata$Category)
mydata_hf <- mydata[(mydata$Category == "BURGLARY" | mydata$Category == "ASSAULT" | mydata$Category =='VANDALISM'),] #mydata$Category =='LARCENY/THEFT',]#
mydata_hf_knn <- data.frame(mydata_hf$Category,mydata_hf$X,mydata_hf$Y)
colnames(mydata_hf_knn) <- c("Category","X","Y")
colnames(mydata_hf_knn)
#mydata_hf_knn
set.seed(100)
n = nrow(mydata_hf_knn)
trainIndex = sample(1:n, size = round(0.80*n), replace=FALSE)
train = mydata_hf_knn[trainIndex ,]
test = mydata_hf_knn[-trainIndex ,]
library(C50)
crime_model<-C5.0(train[-1],train$Category)
crime_model
crime_predict<-predict(crime_model,test)
#install.packages("gmodels")
library(gmodels)
CrossTable(test$Category, crime_predict,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
sum(crime_predict==test$Category)/nrow(test)
library(C50)
crime_boost<-C5.0(train[-1],train$Category,trials = 10)
#summary(crime_boost)
crime_boost_pred10 <- predict(crime_boost, test)
CrossTable(test$Category, crime_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
sum(crime_boost_pred10==test$Category)/nrow(test)
library(randomForest)
library(caret)
#Category <- droplevels(dataset2[dataset2$order=="groupA",])
train_1<-droplevels(train)
test_1<-droplevels(test)
rf_crime <- randomForest(Category ~ ., data = train_1)
rf_crime
rf_predict <- predict(rf_crime, test_1)
CrossTable(test_1$Category, rf_predict,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
sum(rf_predict==test_1$Category) / nrow(test_1)
# Get train and test labels from the dataset
train_labels <- mydata_hf_knn[trainIndex, 1]
test_labels <- mydata_hf_knn[-trainIndex, 1]
# PRE-PROCESSING AND FEATURE ENGINEERING.
# Scale Train X & Y data
x_train_scaled = scale(train$X)
y_train_scaled = scale(train$Y)
# Scale dependent variables in 'test' using mean and standard deviation derived from scaling variables in 'train'.
x_test_scaled = (test$X - attr(x_train_scaled, 'scaled:center')) / attr(x_train_scaled, 'scaled:scale')
y_test_scaled = (test$Y - attr(y_train_scaled, 'scaled:center')) / attr(y_train_scaled, 'scaled:scale')
train_knn <- data.frame(x_train_scaled,y_train_scaled)
test_knn <- data.frame(x_test_scaled,y_test_scaled)
# KNN doesn't do well with Missing Values, so check in train and test dataset before implementing KNN
sum(is.na(train_knn))
sum(is.na(test_knn))
# KNN function is from "class" library
# Implement KNN using knn funtion
library("class")
mydata_test_pred <- knn(train = train_knn, test = test_knn, cl = train_labels, k = 21)
# With the help of Confusion Matrix, we can visualize the performance of a Model.
# Croostable function is from "gmodels" library
library(gmodels)
CrossTable(x = test_labels, y = mydata_test_pred, prop.chisq=FALSE)
# Accuracy = (2377+491+316)/5987 = 53.18 %
